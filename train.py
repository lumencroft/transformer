import torch
import numpy as np
from model import GPT, GPTConfig
import math
import time
import os
import urllib.request
import zipfile

# -----------------------------------------------------------------------------
# 1. ì„¤ì • (Hyperparameters) - ë‹ˆê°€ ë‚˜ì¤‘ì— ë°”ê¿”ì•¼ í•  ìˆ˜ë„ ìˆì–´
# -----------------------------------------------------------------------------
batch_size = 32        # 64 -> 32 (ë©”ëª¨ë¦¬ ë¶€ë‹´ ì¤„ì´ê¸°)
block_size = 128       # 256 -> 128 (Attention ì—°ì‚°ëŸ‰ 4ë°° ê°ì†Œ íš¨ê³¼)
max_iters = 5000  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì§§ê²Œ ì¡ìŒ. ì‹¤ì œë¡  ë” ëŠ˜ë ¤ì•¼ í•´
eval_interval = 500
learning_rate = 1e-3   # ëª¨ë¸ ì‘ì•„ì§€ë‹ˆê¹Œ í•™ìŠµë¥  ì¢€ ë†’ì´ì
device = 'cuda' if torch.cuda.is_available() else 'cpu'
if torch.backends.mps.is_available(): device = 'mps' # ë§¥ë¶ ì“°ëŠ” ê±° ì•„ë‹ˆì§€? í˜¹ì‹œ ëª°ë¼ ë„£ìŒ

# -----------------------------------------------------------------------------
# 2. ë°ì´í„° ì¤€ë¹„ (Data Prep) - "ì•¼ìƒì˜" ë°©ì‹ìœ¼ë¡œ ì§ì ‘ ë¡œë“œ
# -----------------------------------------------------------------------------
data_dir = 'data'
if not os.path.exists(data_dir):
    os.makedirs(data_dir)

file_path = os.path.join(data_dir, 'enwik8')
zip_path = os.path.join(data_dir, 'enwik8.zip')

# íŒŒì¼ì´ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ (Matt Mahoneyì˜ ì›ë³¸ ì‚¬ì´íŠ¸)
if not os.path.exists(file_path):
    print("ğŸ“¥ Downloading enwik8 from source...")
    url = "http://mattmahoney.net/dc/enwik8.zip"
    urllib.request.urlretrieve(url, zip_path)
    print("ğŸ“¦ Unzipping...")
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(data_dir)
else:
    print("âœ… enwik8 already exists. Skipping download.")

# íŒŒì¼ ì½ê¸° (rb ëª¨ë“œ = Raw Bytes)
print("ğŸ“‚ Reading raw bytes...")
with open(file_path, 'rb') as f:
    raw_data = f.read() # bytes íƒ€ì…ìœ¼ë¡œ ì½í˜

# ë°”ì´íŠ¸(0~255)ë¥¼ ì •ìˆ˜ í…ì„œë¡œ ë³€í™˜
# numpyë¥¼ ê±°ì³ì„œ tensorë¡œ ë§Œë“œëŠ” ê²Œ ì†ë„ê°€ ë¹¨ë¼
print("ğŸ”„ Converting to tensor...")
data_tensor = torch.from_numpy(np.frombuffer(raw_data, dtype=np.uint8).copy()).long()

n = len(data_tensor)
# ë¬¸ì œ ì¡°ê±´: 90M Train / 5M Dev / 5M Test
train_data = data_tensor[:90_000_000]
val_data = data_tensor[90_000_000:95_000_000]
test_data = data_tensor[95_000_000:]

print(f"Dataset Split Completed! Total bytes: {n}")
print(f"Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_data)}")

# -----------------------------------------------------------------------------
# 3. ëª¨ë¸ ì´ˆê¸°í™” (Model Init)
# -----------------------------------------------------------------------------
# enwik8ì€ byte ë‹¨ìœ„ë‹ˆê¹Œ vocab_sizeëŠ” ë¬´ì¡°ê±´ 256ì´ì•¼.
config = GPTConfig(
    vocab_size=256, 
    block_size=block_size,
    n_layer=4,      # ë² ì´ìŠ¤ë¼ì¸ì´ë‹ˆê¹Œ ê°€ë³ê²Œ ì‹œì‘
    n_head=4, 
    n_embd=128,
    dropout=0.0,
    use_conv=False
)
model = GPT(config)
model.to(device)
print(f"ğŸ¤– Model initialized with {sum(p.numel() for p in model.parameters())/1e6:.2f}M params")

optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

# -----------------------------------------------------------------------------
# 4. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (Utils)
# -----------------------------------------------------------------------------
def get_batch(split):
    data = train_data if split == 'train' else val_data
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    x, y = x.to(device), y.to(device)
    return x, y

# train.py í•¨ìˆ˜ ìˆ˜ì •
@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(20)  # <--- ì›ë˜ 200ì´ì—ˆìŒ. 20ìœ¼ë¡œ ì¤„ì—¬!
        for k in range(20):       # <--- ì—¬ê¸°ë„ 20ìœ¼ë¡œ!
            X, Y = get_batch(split)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out

# -----------------------------------------------------------------------------
# 5. í•™ìŠµ ë£¨í”„ (Training Loop) - ìˆ˜ë‹¤ìŸì´ ëª¨ë“œ
# -----------------------------------------------------------------------------
print("ğŸš€ Training Start!")
start_time = time.time()

for iter in range(max_iters):
    # ì£¼ê¸°ì ìœ¼ë¡œ í‰ê°€ (ì˜¤ë˜ ê±¸ë¦¼)
    if iter % eval_interval == 0:
        elapsed = time.time() - start_time
        losses = estimate_loss()
        train_bpc = losses['train'] / math.log(2)
        val_bpc = losses['val'] / math.log(2)
        print(f"\n[Step {iter}] time: {elapsed:.2f}s | train: {train_bpc:.4f} bpc | val: {val_bpc:.4f} bpc")

    # í•™ìŠµ ì§„í–‰ (ì—¬ê¸°ì„œ ë©ˆì¶˜ ê²ƒì²˜ëŸ¼ ë³´ì˜€ë˜ ê±°ì„)
    xb, yb = get_batch('train')
    logits, loss = model(xb, yb)
    
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()
    
    # â˜…â˜…â˜… ìƒì¡´ ì‹ ê³  ì¶”ê°€ â˜…â˜…â˜…
    # ì¤„ë°”ê¿ˆ ì—†ì´ ì (.)ë§Œ ì°ì–´ì„œ ì§„í–‰ ìƒí™© ë³´ì—¬ì¤Œ
    print(".", end="", flush=True) 

print("\nğŸ Training Finished!")